---
title: "Classification"
author: "Sohyun Park"
editor: visual
---

## Summary

### How to classify remotely sensed data

#### Classification trees

-   Classify data into two or more discrete categories

-   Output variable is categoriesed such as religion, sex, nationality

-   Top-down approach

-   The final leaves can be a mixture of the categories(impure), so we quantify this with the Gini Impurity

    ![](images/paste-E98859C4.png)

Source:[An Introduction to Data Science, Dr Saed Sayad](https://www.saedsayad.com/decision_tree.htm)

#### Regression trees

-   Regression trees is decision tree model with regression

-   Output variable is continuous values such as height, weight or temperature

-   Bottom-up approach

-   Subset the data into smaller chunks

    ![](images/paste-F95DFBD4.png)

    Source:[Luka Beverin](https://medium.datadriveninvestor.com/how-do-regression-trees-work-94999c5105d)

#### Overfitting

-   What if we have a leaf with just one person or one pixel value? = **overfitting**

    -   Bias means difference between predicted value and true value(**oversimplifies model)**

    -   Variance is variability of model for a given point

![](images/paste-F0B4E05F.png)

Source:[Seema Singh](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)

#### Prevent overfitting

-   We can fix it by limiting how trees grow, for example we can set a minimum number of pixels in a leaf(20 is often used)

    ![](https://ucfnoix.github.io/CASA0023_Learning_Diary/images/decision-tree-subtrees.png)Source:[ML Wiki](http://mlwiki.org/index.php/Cost-Complexity_Pruning)

-   Also we can fix it by **weakest link pruning** (with tree score)

    -   use one less leaf, remove a leaf = **sub-tree**, SSR will get larger = **termed PRUNING or cost complexity pruning**

    -   Sum for the tree

    -   Tree score = SSR + tree penalty (alpha) \* T (number of leaves) **Lower means better**

```{=html}
<!-- -->
```
-   There is another method... **Random Forest**

    -   Ensemble machine learning model

    -   All decision trees are grown by randomly extracting sub-datasets

    -   A single data may be selected multiple times as it allows redundancy

    -   Find the overall value which got more values based on all the trees

        ![](images/paste-CF6CAF04.png)

Source:[Rosaria Silipo](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)

### Image classification

There are three types of image classification.

| Supervised Classification                                             | Unsupervised Classification |
|-------------------------------------------|---------------------------------------|
| Pattern recognition or machine learning                               | clustering                  |
| Parametric (normal distribution) : Maximum likelihood                 | k-means                     |
| non parametric (not normal) : Support Vector Machine, Neural Networks | ISODATA                     |

![](images/paste-0BFC54B9.png)

Source:[GIS Geography](https://gisgeography.com/supervised-unsupervised-classification-arcgis/)

## Applications

## Personal Reflection

As a surveying engineer, I have always been obsessed with accurate figures. This background is why it is important to choose the model with the highest accuracy. And even in the real world, it's common to prioritize accuracy because most of us aim for profit. In this week's lesson, I learned that classification methods fundamentally analyze data differently, which affects the ability to build accurate models. Although I did not fully understand the mathematical principles of each classification method, my goal is to build a model that is close to perfection by understanding it more clearly. However, in the field of social science research, since there is a possibility of understanding a phenomenon and inducing certain results rather than simply accurately predicting a phenomenon, interpretability should also be considered.
